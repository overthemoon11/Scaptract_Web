# Facial Expression Recognition using Vanilla ViT backbones with MAE Pretraining 

Jia Li 

School of Computer Science and Information Engineering, Hefei University of Technology 

jiali@hfut.edu.cn 



## Abstract 

Humans usually convey emotions voluntarily or involuntarily by facial expressions. Automatically recognizing the basic expression (such as happiness, sadness, and neutral) from a facial image, i.e., facial expression recognition (FER), is extremely challenging and attracts much research interests.Large scale datasets and powerful inference models have been proposed to address the problem. Though considerable progress has been made, most of the state of the arts employing convolutional neural networks (CNNs) or elaborately modified Vision Transformers (ViTs) depend heavily on upstream supervised pretraining. Transformers are taking place the domination of CNNs in more and more computer vision tasks. But they usually need much more data to train, since they use less inductive biases compared with CNNs. To explore whether a vanilla ViT without extra training samples from upstream tasks is able to achieve competitive accuracy,we use a plain ViT with MAE pretraining to perform the FER task. Specifically, we first pretrain the original ViT as a Masked Autoencoder (MAE) on a large facial expression dataset without expression labels. Then, we fine-tune the ViT on popular facial expression datasets with expression labels.The presented method is quite competitive with 91.07% on RAF-DB, 62.42% on AfectNet, and 90.18% on FER-Plus,which can serve as a simple yet strong ViT-based baseline for FER studies. The code and pretrained models will be publicly available.


## I ntroduction 

Facial expression analysis plays an important role in understanding one's feelings, attitudes and intensions (Gong, An,and Elfiky 2022; Zhao, Liu, and Zhou 2021), which facilitate many other techniques such as new-type human-computer interaction and mental health assessment. The task of facial expression recognition (FeR) aims to infer the basic emotion class, such as happiness, sadness, and anger, of the person in a given facial image. Researches have made substantial progress in the past few years, benefiting from the development of more powerful deep neural networks, such as ResNet (He et al. 2016) and Vision Transformer (Dosovitskiy et al. 2020), and more representative benchmarks, such as AffencNet (Mollahosseini, Hasani, and Mahoor 2017)and RAF-DB (Li, Deng, and Du 2017).



Predicting the discrete emotion class of a facial image is difficult due to the expression ambiguity, imbalanced and noisy emotion labels. Therefore, most of the state-of-the-art (SOTA) methods (Xue, Wang, and Guo 2021; Li et al. 2021;She et al. 2021; Hwang et al. 2022; Wang et al. 2020) commonly pretrain deep models on large scale and less noisy datasets, for instance, Ms-Celeb-1M (Guo et al. 2016) and even ImageNet, to gain better feature representations first and fine-tune deep models on specific FER datasets. Most recently, CNNs equipped with attention modules (Ma, Sun,and Li 2021; Xue, Wang, and Guo 2021) and ViT variants (Li et al. 2021) have been introduced into FER and obtained promising performance. However, whether a vanilla ViT without extra training samples from upstream tasks can achieve competitive performance has not been explored thus far. In this paper, we demonstrate that a vanilla ViT-base backbone pretrained by MAE method (He et al. 2022) without extra data is comparable to the SOTA work.

## Our Approach 

We initialize the original ViT as a Masked Autoencoder (MAE) (He et al. 2022) on AffectNet (Mollahosseini,Hasani, and Mahoor 2017) dataset such that it can extract a stronger representation of expressive semantic information. MAE pretraining force the ViT to learn the facial local and global structures (Chen et al. 2022). Different facial emotions often share one or more facial muscle movements.Thus, facial expressions with low occurrence frequencies,for example, fear, could benefit from the structure learning of frequent facial expressions, for example, happiness, during the pretraining.



MAE. The masked autoencoder (He et al. 2022) employs an asymmetric encoder-decoder architecture to implement the self-supervised task by randomly masking the patches of input images and reconstructing missing pixels. It is experimentally demonstrated that MAE can learn a large number of visual semantics and has excellent performance in several downstream tasks. Inspired by this, we adopt MAE to pretrain a vanilla ViT with powerful expression semantic representation and narrow the gap between self-supervised task and downstream FER task.



Dataset. The upstream pretraining task requires large