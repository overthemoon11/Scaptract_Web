{
  "metadata": {
    "source": "PPStructureV3",
    "total_length": 12954,
    "line_count": 104
  },
  "content": {
    "Facial Expression Recognition using Vanilla ViT backbones with MAE Pretraining ": {
      "Abstract ": "Humans usually convey emotions voluntarily or involuntarily by facial expressions. Automatically recognizing the basic expression (such as happiness, sadness, and neutral) from a facial image, i.e.,facial expression recognition (FER), is extremely challenging and attracts much research interests.Large scale datasets and powerful inference models have been proposed to address the problem. Though considerable progress has been made, most of the state of the arts employing convolutional neural networks (CNNs) or elaborately modified Vision Transformers (ViTs) depend heavily on upstream supervised pretraining. Transformers are taking place the domination of CNNs in more and more computer vision tasks. But they usually need much more data to train, since they use less inductive biases compared with CNNs. To explore whether a vanilla ViT without extra training samples from upstream tasks is able to achieve competitive accuracy,we use a plain ViT with MAE pretraining to perform the FER task. Specifically, we first pretrain the original ViT as a Masked Autoencoder (MAE) on a large facial expression dataset without expression labels. Then, we fine-tune the ViT on popular facial expression datasets with expression labels.The presented method is quite competitive with 91.07% on RAF-DB, 62.42% on AfectNet, and 90.18% on FER-Plus,which can serve as a simple yet strong ViT-based baseline for FER studies. The code and pretrained models will be publicly available.",
      "I ntroduction ": "Facial expression analysis plays an important role in understanding one's feelings, attitudes and intensions (Gong, An,and Elfiky 2022; Zhao, Liu, and Zhou 2021), which facilitate many other techniques such as new-type human-computer interaction and mental health assessment. The task of facial expression recognition (FeR) aims to infer the basic emotion class, such as happiness, sadness, and anger, of the person in a given facial image. Researches have made substantial progress in the past few years, benefiting from the development of more powerful deep neural networks, such as ResNet (He et al. 2016) and Vision Transformer (Dosovitskiy et al. 2020), and more representative benchmarks, such as AffencNet (Mollahosseini, Hasani, and Mahoor 2017)and RAF-DB (Li, Deng, and Du 2017).\n\nPredicting the discrete emotion class of a facial image is difficult due to the expression ambiguity, imbalanced and noisy emotion labels. Therefore, most of the state-of-the-art (SOTA) methods (Xue, Wang, and Guo 2021; Li et al. 2021;She et al. 2021; Hwang et al. 2022; Wang et al. 2020) commonly pretrain deep models on large scale and less noisy datasets, for instance, Ms-Celeb-1M (Guo et al. 2016) and even ImageNet, to gain better feature representations first and fine-tune deep models on specific FER datasets. Most recently, CNNs equipped with attention modules (Ma, Sun,and Li 2021; Xue, Wang, and Guo 2021) and ViT variants (Li et al. 2021) have been introduced into FER and obtained promising performance. However, whether a vanilla ViT without extra training samples from upstream tasks can achieve competitive performance has not been explored thus far. In this paper, we demonstrate that a vanilla ViT-base backbone pretrained by MAE method (He et al. 2022) without extra data is comparable to the SOTA work.",
      "Our Approach ": "We initialize the original ViT as a Masked Autoencoder (MAE) (He et al. 2022) on AffectNet (Mollahosseini,Hasani, and Mahoor 2017) dataset such that it can extract a stronger representation of expressive semantic information. MAE pretraining force the ViT to learn the facial local and global structures (Chen et al. 2022). Different facial emotions often share one or more facial muscle movements.Thus, facial expressions with low occurrence frequencies,for example, fear, could benefit from the structure learning of frequent facial expressions, for example, happiness, during the pretraining.\n\nMAE. The masked autoencoder (He et al. 2022) employs an asymmetric encoder-decoder architecture to implement the self-supervised task by randomly masking the patches of input images and reconstructing missing pixels. It is experimentally demonstrated that MAE can learn a large number of visual semantics and has excellent performance in several downstream tasks. Inspired by this, we adopt MAE to pretrain a vanilla ViT with powerful expression semantic representation and narrow the gap between self-supervised task and downstream FER task \n\nDataset. The upstream pretraining task requires large\n\nscale data, which enables the model to have strong representational capailii.For R,AfctNt istelarstavailable in-the-wild dataset that contains more than one million face images collected from the Internet by keyword searching. Among them, about 400K images are manually labeled.We clean the dataset based on the label file and use OpenFace (Baltrusaitis et al. 2018) for face alignment, leaving 269,316 images for self-supervised training.",
      "Experiment ": "",
      "I mplementation ": "We choose the ViT-base and do self-supervised pretraining on the AffectNet training set. The pretraining length is 300epochs and the other settings of pretraining and fine-tuning are same as MAE (He et al. 2022). Due to the tranining set of AffectNet is imbalanced whereas the validation set is balanced, oversampling strategy is used while fine-tuning. We train the ViT model on each FER training subset and then evaluate on each FER testing subset.",
      "Result ": "We compare our method with two pretraining methods using supervised learning and MAE on ImageNet-1K in Table 1. Although AffectNet is almost 4× smaller than ImageNet1K, pretraining with MAE on AffectNet yields better fine-tuning results than both supervised learning and MAE pretraining methods on ImageNet-1K. Our method based on ViT-base achieves the best accuracy of 90.22%, 61.73% and 89.00% on RAF-DB, AffectNet and FER-Plus respectively without any data augmentation, which gives us a powerful pretraining model for the subsequent work. And the performance reaches 91.07% on RAF-DB, 62.42% on AfectNet,and 90.18% on FER-Plus if regular augmentation is used (random horizontal flipping, color jitter and affine transformation).\n\n[TABLE_0]\n\n[TABLE_1]\n\nIn addition, to demonstrate the effectiveness of our method, we compare it with the existing state-of-the-art methods. In Table2, SCN (Wang et al. 2020) and RUL (Zhang, Wang, and Deng 2021) adopt ResNet as the backbone and introduce different uncertainty learning methods to tackle the uncertainty problem in FER. MVT (Li et al.2021) proposes the combination of grid-wise local attention and token-based global attention to overcome the weakness of CNN-based in long-range relation modeling. Compared with these well-designed SOTA methods, our results on the three FER datasets demonstrate the effectiveness and scalability of our method.\n\n[TABLE_2]",
      "Conclusions ": "In this paper, we demonstrate that a vanilla ViT-based backbone pretrained by MAE method (He et al. 2022) without extra data is comparable to the SOTA work. The presented method achieves 91.07% on RAF-DB, 62.42% on AfectNet,and 90.18% on FER-Plus, which can serve as a transparent ViT-based baseline for FER studies.",
      "References ": "Baltrusaitis, T.; Zadeh, A.; Lim, Y. C.; and Morency, L.-P. 2018.Openface 2.0: Facial behavior analysis toolkit. In 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018), 59–66. IEEE.\nChen, X.; Ding, M.; Wang, X.; Xin, Y.;Mo, S.;Wang,Y.; Han,S.; Luo,P.; Zeng,G.; and Wang, J. 2022. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026.\nDosovitskiy,A.;Beyer,L.;Kolesnikov,A.;Weissenborn,D.;Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer,M.; Heigold,G.:Gelly,S.;et al. 2020. An image is worth 16x16words:Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\nGong, W.; An, Z.; and Elfiky, N. M. 2022. Deep learning-based microexpression recognition: a survey. Neural Computing and Applications 1–24.\nGuo, Y.; Zhang, L.; Hu, Y.; He, X.; and Gao, J. 2016. Ms-celeb1m: A dataset and benchmark for large-scale face recognition. In European conference on computer vision,, 87–102. Springer.He, K.; Zhang, X.; Ren, S.; and Sun, J. 201. Deep residual learning for image recognition. In Proceedings of the EEE conference on computer vision and pattern recognition, 770–778.He, K.; Chen, X.; Xie, S.; Li,Y.; Dollár, P.; and Girshick, R. 2022.Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 16000–16009.\nHwang, H.; Kim,S.; Park, W.-J.; Seo,J.; Ko, K.; and Yeo, H. 2022.Vision transformer equipped with neural resizer on facial expression recognition task. In ICASSP 2022-2022 IEEE International \n\nConferenceAcouticecndgalcesinCASSP),2614–2618. IEEE.\nLi,H.; Sui, M; Zhao,F;Zha,Z.;and Wu,F.2021.Mvt: mask vision preprint arXiv:2106.04520.\nLi,S.; Deng, W; and Du,J.2017. Rliablecrowdsourcing and dep wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2852–2861.\nMa,F;un.nLi.21.Fclxogto with visual transformers and atentional selectivefusion.IEEE Transactions on Affective Computing.\nMollahossei,A.; Haai,B.;andMaoorM.H.2017.ffectnet:A thewild.EaaciononfctiveComputi):18–31.She,J.;Hu,Y.;,H.;Wa,J.;Q.;ad,T..Dive intoambigut:iii rucr estimationfor facial expression recogition.In roceedingsof e IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6248–6257.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles, A.;and Jégou, H. 2021. Training data-efficient image transformers & distillation through attention. In International conferenceon machine learning, 10347–10357. PMLR.\nWang, K.; Peng, X.;Yang,J.; Lu,S.; and Qiao,Y.2020.Suess ing uncertainties for large-scale facial expression recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 6897–6906.\nXue, F.; Wang, Q.; and Guo, G. 2021. Transfer: Learning relationaware facial expression representations with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 3601–3610.\nZhang, Y.; Wang, C.; and Deng, W. 2021. Relative uncertainty learning for facial expression recognition. Advances in Neural Information Processing Systems 34.\nZhao, Z.; Liu, Q.; and Zhou, F. 2021. Robust lightweight facial expressioncognitionetwork withlabel diibutiontraning.In Proceedingsof the AAAlconferenceonartificial intelligence, volume 35, 3510–3519."
    }
  },
  "structured_elements": {
    "tables": [
      {
        "type": "table",
        "html": "<div style=\"text-align: center;\">Table 1: MAE pretraining vs. supervised pretraining, numbers evaluated by fine-tuning the ViT-base weights from ImageNet-1K and AffectNet on downstream FER datasets without data augmentation. </div>\n\n\n\n<div style=\"text-align: center;\"><html><body><table border=\"1\"><thead><tr><td>method</td><td>pretrain data</td><td>RAF-DB</td><td>AffectNet</td></tr></thead><tbody><tr><td>Supervised</td><td>IN1K</td><td>85.14%</td><td>57.10%</td></tr><tr><td>MAE</td><td>IN1K</td><td>88.49%</td><td>60.87%</td></tr><tr><td>MAE</td><td>AffectNet</td><td>90.22%</td><td>61.73%</td></tr></tbody></table></body></html></div>",
        "rows": [
          [
            "method",
            "pretrain data",
            "RAF-DB",
            "AffectNet"
          ],
          [
            "Supervised",
            "IN1K",
            "85.14%",
            "57.10%"
          ],
          [
            "MAE",
            "IN1K",
            "88.49%",
            "60.87%"
          ],
          [
            "MAE",
            "AffectNet",
            "90.22%",
            "61.73%"
          ]
        ],
        "row_count": 4,
        "column_count": 4
      },
      {
        "type": "table",
        "html": "<div style=\"text-align: center;\">Table 2: More ViT-based results on RAF-DB, AffectNet and FER-Plus. Here, data augmentation is employed. The variants, ViT-small and ViT-tiny are proposed in DeiT (Touvron et al. 2021). </div>\n\n\n\n<div style=\"text-align: center;\"><html><body><table border=\"1\"><thead><tr><td>method</td><td>pretrain data</td><td>ViT variants</td><td>RAF-DB</td><td>AffectNet</td><td>FER-Plus</td></tr></thead><tbody><tr><td>Supervised</td><td>IN1K</td><td>ViT-samll</td><td>87.19%</td><td>57.91%</td><td>88.46%</td></tr><tr><td>Supervised</td><td>IN1K</td><td>ViT-tiny</td><td>87.03%</td><td>58.28%</td><td>88.56%</td></tr><tr><td>Supervised</td><td>IN1K</td><td>ViT-base</td><td>87.22%</td><td>57.99%</td><td>88.91%</td></tr><tr><td>MAE</td><td>AffectNet</td><td>Vit-base</td><td>91.07%</td><td>62.42%</td><td>90.18%</td></tr></tbody></table></body></html></div>",
        "rows": [
          [
            "method",
            "pretrain data",
            "ViT variants",
            "RAF-DB",
            "AffectNet",
            "FER-Plus"
          ],
          [
            "Supervised",
            "IN1K",
            "ViT-samll",
            "87.19%",
            "57.91%",
            "88.46%"
          ],
          [
            "Supervised",
            "IN1K",
            "ViT-tiny",
            "87.03%",
            "58.28%",
            "88.56%"
          ],
          [
            "Supervised",
            "IN1K",
            "ViT-base",
            "87.22%",
            "57.99%",
            "88.91%"
          ],
          [
            "MAE",
            "AffectNet",
            "Vit-base",
            "91.07%",
            "62.42%",
            "90.18%"
          ]
        ],
        "row_count": 5,
        "column_count": 6
      },
      {
        "type": "table",
        "html": "<div style=\"text-align: center;\">Table 3: Comparsion with the state of the art on RAF-DB,AffectNet and FER-Plus. </div>\n\n\n\n<div style=\"text-align: center;\"><html><body><table border=\"1\"><thead><tr><td>method</td><td>year</td><td>RAF-DB</td><td>AffectNet</td><td>FER-Plus</td></tr></thead><tbody><tr><td>SCN (Wang et al. 2020)</td><td>2020</td><td>88.14%</td><td>60.23%</td><td>89.35%</td></tr><tr><td>RUL (Zhang, Wang, and Deng 2021)</td><td>2021</td><td>88.98%</td><td>61.43%</td><td></td></tr><tr><td>MVT (Li et al. 2021)</td><td>2021</td><td>88.26%</td><td>61.40%</td><td>89.22%</td></tr><tr><td>Ours</td><td>2022</td><td>91.07%</td><td>62.42%</td><td>90.18%</td></tr></tbody></table></body></html></div>",
        "rows": [
          [
            "method",
            "year",
            "RAF-DB",
            "AffectNet",
            "FER-Plus"
          ],
          [
            "SCN (Wang et al. 2020)",
            "2020",
            "88.14%",
            "60.23%",
            "89.35%"
          ],
          [
            "RUL (Zhang, Wang, and Deng 2021)",
            "2021",
            "88.98%",
            "61.43%",
            ""
          ],
          [
            "MVT (Li et al. 2021)",
            "2021",
            "88.26%",
            "61.40%",
            "89.22%"
          ],
          [
            "Ours",
            "2022",
            "91.07%",
            "62.42%",
            "90.18%"
          ]
        ],
        "row_count": 5,
        "column_count": 5
      }
    ],
    "images": [],
    "equations": [],
    "sections": [
      {
        "type": "section",
        "level": 1,
        "title": "Facial Expression Recognition using Vanilla ViT backbones with MAE Pretraining",
        "content": "Jia Li \n\nSchool of Computer Science and Information Engineering, Hefei University of Technology \n\njiali@hfut.edu.cn"
      },
      {
        "type": "section",
        "level": 2,
        "title": "Abstract",
        "content": "Humans usually convey emotions voluntarily or involuntarily by facial expressions. Automatically recognizing the basic expression (such as happiness, sadness, and neutral) from a facial image, i.e.,facial expression recognition (FER), is extremely challenging and attracts much research interests.Large scale datasets and powerful inference models have been proposed to address the problem. Though considerable progress has been made, most of the state of the arts employing convolutional neural networks (CNNs) or elaborately modified Vision Transformers (ViTs) depend heavily on upstream supervised pretraining. Transformers are taking place the domination of CNNs in more and more computer vision tasks. But they usually need much more data to train, since they use less inductive biases compared with CNNs. To explore whether a vanilla ViT without extra training samples from upstream tasks is able to achieve competitive accuracy,we use a plain ViT with MAE pretraining to perform the FER task. Specifically, we first pretrain the original ViT as a Masked Autoencoder (MAE) on a large facial expression dataset without expression labels. Then, we fine-tune the ViT on popular facial expression datasets with expression labels.The presented method is quite competitive with 91.07% on RAF-DB, 62.42% on AfectNet, and 90.18% on FER-Plus,which can serve as a simple yet strong ViT-based baseline for FER studies. The code and pretrained models will be publicly available."
      },
      {
        "type": "section",
        "level": 2,
        "title": "I ntroduction",
        "content": "Facial expression analysis plays an important role in understanding one's feelings, attitudes and intensions (Gong, An,and Elfiky 2022; Zhao, Liu, and Zhou 2021), which facilitate many other techniques such as new-type human-computer interaction and mental health assessment. The task of facial expression recognition (FeR) aims to infer the basic emotion class, such as happiness, sadness, and anger, of the person in a given facial image. Researches have made substantial progress in the past few years, benefiting from the development of more powerful deep neural networks, such as ResNet (He et al. 2016) and Vision Transformer (Dosovitskiy et al. 2020), and more representative benchmarks, such as AffencNet (Mollahosseini, Hasani, and Mahoor 2017)and RAF-DB (Li, Deng, and Du 2017).\n\n\n\nPredicting the discrete emotion class of a facial image is difficult due to the expression ambiguity, imbalanced and noisy emotion labels. Therefore, most of the state-of-the-art (SOTA) methods (Xue, Wang, and Guo 2021; Li et al. 2021;She et al. 2021; Hwang et al. 2022; Wang et al. 2020) commonly pretrain deep models on large scale and less noisy datasets, for instance, Ms-Celeb-1M (Guo et al. 2016) and even ImageNet, to gain better feature representations first and fine-tune deep models on specific FER datasets. Most recently, CNNs equipped with attention modules (Ma, Sun,and Li 2021; Xue, Wang, and Guo 2021) and ViT variants (Li et al. 2021) have been introduced into FER and obtained promising performance. However, whether a vanilla ViT without extra training samples from upstream tasks can achieve competitive performance has not been explored thus far. In this paper, we demonstrate that a vanilla ViT-base backbone pretrained by MAE method (He et al. 2022) without extra data is comparable to the SOTA work."
      },
      {
        "type": "section",
        "level": 2,
        "title": "Our Approach",
        "content": "We initialize the original ViT as a Masked Autoencoder (MAE) (He et al. 2022) on AffectNet (Mollahosseini,Hasani, and Mahoor 2017) dataset such that it can extract a stronger representation of expressive semantic information. MAE pretraining force the ViT to learn the facial local and global structures (Chen et al. 2022). Different facial emotions often share one or more facial muscle movements.Thus, facial expressions with low occurrence frequencies,for example, fear, could benefit from the structure learning of frequent facial expressions, for example, happiness, during the pretraining.\n\n\n\nMAE. The masked autoencoder (He et al. 2022) employs an asymmetric encoder-decoder architecture to implement the self-supervised task by randomly masking the patches of input images and reconstructing missing pixels. It is experimentally demonstrated that MAE can learn a large number of visual semantics and has excellent performance in several downstream tasks. Inspired by this, we adopt MAE to pretrain a vanilla ViT with powerful expression semantic representation and narrow the gap between self-supervised task and downstream FER task \n\n\n\nDataset. The upstream pretraining task requires large\n\nscale data, which enables the model to have strong representational capailii.For R,AfctNt istelarstavailable in-the-wild dataset that contains more than one million face images collected from the Internet by keyword searching. Among them, about 400K images are manually labeled.We clean the dataset based on the label file and use OpenFace (Baltrusaitis et al. 2018) for face alignment, leaving 269,316 images for self-supervised training."
      },
      {
        "type": "section",
        "level": 2,
        "title": "Experiment",
        "content": ""
      },
      {
        "type": "section",
        "level": 2,
        "title": "I mplementation",
        "content": "We choose the ViT-base and do self-supervised pretraining on the AffectNet training set. The pretraining length is 300epochs and the other settings of pretraining and fine-tuning are same as MAE (He et al. 2022). Due to the tranining set of AffectNet is imbalanced whereas the validation set is balanced, oversampling strategy is used while fine-tuning. We train the ViT model on each FER training subset and then evaluate on each FER testing subset."
      },
      {
        "type": "section",
        "level": 2,
        "title": "Result",
        "content": "We compare our method with two pretraining methods using supervised learning and MAE on ImageNet-1K in Table 1. Although AffectNet is almost 4× smaller than ImageNet1K, pretraining with MAE on AffectNet yields better fine-tuning results than both supervised learning and MAE pretraining methods on ImageNet-1K. Our method based on ViT-base achieves the best accuracy of 90.22%, 61.73% and 89.00% on RAF-DB, AffectNet and FER-Plus respectively without any data augmentation, which gives us a powerful pretraining model for the subsequent work. And the performance reaches 91.07% on RAF-DB, 62.42% on AfectNet,and 90.18% on FER-Plus if regular augmentation is used (random horizontal flipping, color jitter and affine transformation).\n\n\n\n<div style=\"text-align: center;\">Table 1: MAE pretraining vs. supervised pretraining, numbers evaluated by fine-tuning the ViT-base weights from ImageNet-1K and AffectNet on downstream FER datasets without data augmentation. </div>\n\n\n\n<div style=\"text-align: center;\"><html><body><table border=\"1\"><thead><tr><td>method</td><td>pretrain data</td><td>RAF-DB</td><td>AffectNet</td></tr></thead><tbody><tr><td>Supervised</td><td>IN1K</td><td>85.14%</td><td>57.10%</td></tr><tr><td>MAE</td><td>IN1K</td><td>88.49%</td><td>60.87%</td></tr><tr><td>MAE</td><td>AffectNet</td><td>90.22%</td><td>61.73%</td></tr></tbody></table></body></html></div>\n\n\n<div style=\"text-align: center;\">Table 2: More ViT-based results on RAF-DB, AffectNet and FER-Plus. Here, data augmentation is employed. The variants, ViT-small and ViT-tiny are proposed in DeiT (Touvron et al. 2021). </div>\n\n\n\n<div style=\"text-align: center;\"><html><body><table border=\"1\"><thead><tr><td>method</td><td>pretrain data</td><td>ViT variants</td><td>RAF-DB</td><td>AffectNet</td><td>FER-Plus</td></tr></thead><tbody><tr><td>Supervised</td><td>IN1K</td><td>ViT-samll</td><td>87.19%</td><td>57.91%</td><td>88.46%</td></tr><tr><td>Supervised</td><td>IN1K</td><td>ViT-tiny</td><td>87.03%</td><td>58.28%</td><td>88.56%</td></tr><tr><td>Supervised</td><td>IN1K</td><td>ViT-base</td><td>87.22%</td><td>57.99%</td><td>88.91%</td></tr><tr><td>MAE</td><td>AffectNet</td><td>Vit-base</td><td>91.07%</td><td>62.42%</td><td>90.18%</td></tr></tbody></table></body></html></div>\n\n\nIn addition, to demonstrate the effectiveness of our method, we compare it with the existing state-of-the-art methods. In Table2, SCN (Wang et al. 2020) and RUL (Zhang, Wang, and Deng 2021) adopt ResNet as the backbone and introduce different uncertainty learning methods to tackle the uncertainty problem in FER. MVT (Li et al.2021) proposes the combination of grid-wise local attention and token-based global attention to overcome the weakness of CNN-based in long-range relation modeling. Compared with these well-designed SOTA methods, our results on the three FER datasets demonstrate the effectiveness and scalability of our method.\n\n\n\n<div style=\"text-align: center;\">Table 3: Comparsion with the state of the art on RAF-DB,AffectNet and FER-Plus. </div>\n\n\n\n<div style=\"text-align: center;\"><html><body><table border=\"1\"><thead><tr><td>method</td><td>year</td><td>RAF-DB</td><td>AffectNet</td><td>FER-Plus</td></tr></thead><tbody><tr><td>SCN (Wang et al. 2020)</td><td>2020</td><td>88.14%</td><td>60.23%</td><td>89.35%</td></tr><tr><td>RUL (Zhang, Wang, and Deng 2021)</td><td>2021</td><td>88.98%</td><td>61.43%</td><td></td></tr><tr><td>MVT (Li et al. 2021)</td><td>2021</td><td>88.26%</td><td>61.40%</td><td>89.22%</td></tr><tr><td>Ours</td><td>2022</td><td>91.07%</td><td>62.42%</td><td>90.18%</td></tr></tbody></table></body></html></div>"
      },
      {
        "type": "section",
        "level": 2,
        "title": "Conclusions",
        "content": "In this paper, we demonstrate that a vanilla ViT-based backbone pretrained by MAE method (He et al. 2022) without extra data is comparable to the SOTA work. The presented method achieves 91.07% on RAF-DB, 62.42% on AfectNet,and 90.18% on FER-Plus, which can serve as a transparent ViT-based baseline for FER studies."
      },
      {
        "type": "section",
        "level": 2,
        "title": "References",
        "content": "Baltrusaitis, T.; Zadeh, A.; Lim, Y. C.; and Morency, L.-P. 2018.Openface 2.0: Facial behavior analysis toolkit. In 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018), 59–66. IEEE.\nChen, X.; Ding, M.; Wang, X.; Xin, Y.;Mo, S.;Wang,Y.; Han,S.; Luo,P.; Zeng,G.; and Wang, J. 2022. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026.\nDosovitskiy,A.;Beyer,L.;Kolesnikov,A.;Weissenborn,D.;Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer,M.; Heigold,G.:Gelly,S.;et al. 2020. An image is worth 16x16words:Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\nGong, W.; An, Z.; and Elfiky, N. M. 2022. Deep learning-based microexpression recognition: a survey. Neural Computing and Applications 1–24.\nGuo, Y.; Zhang, L.; Hu, Y.; He, X.; and Gao, J. 2016. Ms-celeb1m: A dataset and benchmark for large-scale face recognition. In European conference on computer vision,, 87–102. Springer.He, K.; Zhang, X.; Ren, S.; and Sun, J. 201. Deep residual learning for image recognition. In Proceedings of the EEE conference on computer vision and pattern recognition, 770–778.He, K.; Chen, X.; Xie, S.; Li,Y.; Dollár, P.; and Girshick, R. 2022.Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 16000–16009.\nHwang, H.; Kim,S.; Park, W.-J.; Seo,J.; Ko, K.; and Yeo, H. 2022.Vision transformer equipped with neural resizer on facial expression recognition task. In ICASSP 2022-2022 IEEE International \n\nConferenceAcouticecndgalcesinCASSP),2614–2618. IEEE.\nLi,H.; Sui, M; Zhao,F;Zha,Z.;and Wu,F.2021.Mvt: mask vision preprint arXiv:2106.04520.\nLi,S.; Deng, W; and Du,J.2017. Rliablecrowdsourcing and dep wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2852–2861.\nMa,F;un.nLi.21.Fclxogto with visual transformers and atentional selectivefusion.IEEE Transactions on Affective Computing.\nMollahossei,A.; Haai,B.;andMaoorM.H.2017.ffectnet:A thewild.EaaciononfctiveComputi):18–31.She,J.;Hu,Y.;,H.;Wa,J.;Q.;ad,T..Dive intoambigut:iii rucr estimationfor facial expression recogition.In roceedingsof e IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6248–6257.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles, A.;and Jégou, H. 2021. Training data-efficient image transformers & distillation through attention. In International conferenceon machine learning, 10347–10357. PMLR.\nWang, K.; Peng, X.;Yang,J.; Lu,S.; and Qiao,Y.2020.Suess ing uncertainties for large-scale facial expression recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 6897–6906.\nXue, F.; Wang, Q.; and Guo, G. 2021. Transfer: Learning relationaware facial expression representations with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 3601–3610.\nZhang, Y.; Wang, C.; and Deng, W. 2021. Relative uncertainty learning for facial expression recognition. Advances in Neural Information Processing Systems 34.\nZhao, Z.; Liu, Q.; and Zhou, F. 2021. Robust lightweight facial expressioncognitionetwork withlabel diibutiontraning.In Proceedingsof the AAAlconferenceonartificial intelligence, volume 35, 3510–3519."
      }
    ],
    "table_count": 3,
    "image_count": 0,
    "equation_count": 0,
    "section_count": 9
  },
  "raw_markdown": "\n\n# Facial Expression Recognition using Vanilla ViT backbones with MAE Pretraining \n\nJia Li \n\nSchool of Computer Science and Information Engineering, Hefei University of Technology \n\njiali@hfut.edu.cn \n\n\n\n## Abstract \n\nHumans usually convey emotions voluntarily or involuntarily by facial expressions. Automatically recognizing the basic expression (such as happiness, sadness, and neutral) from a facial image, i.e.,facial expression recognition (FER), is extremely challenging and attracts much research interests.Large scale datasets and powerful inference models have been proposed to address the problem. Though considerable progress has been made, most of the state of the arts employing convolutional neural networks (CNNs) or elaborately modified Vision Transformers (ViTs) depend heavily on upstream supervised pretraining. Transformers are taking place the domination of CNNs in more and more computer vision tasks. But they usually need much more data to train, since they use less inductive biases compared with CNNs. To explore whether a vanilla ViT without extra training samples from upstream tasks is able to achieve competitive accuracy,we use a plain ViT with MAE pretraining to perform the FER task. Specifically, we first pretrain the original ViT as a Masked Autoencoder (MAE) on a large facial expression dataset without expression labels. Then, we fine-tune the ViT on popular facial expression datasets with expression labels.The presented method is quite competitive with 91.07% on RAF-DB, 62.42% on AfectNet, and 90.18% on FER-Plus,which can serve as a simple yet strong ViT-based baseline for FER studies. The code and pretrained models will be publicly available.\n\n\n## I ntroduction \n\nFacial expression analysis plays an important role in understanding one's feelings, attitudes and intensions (Gong, An,and Elfiky 2022; Zhao, Liu, and Zhou 2021), which facilitate many other techniques such as new-type human-computer interaction and mental health assessment. The task of facial expression recognition (FeR) aims to infer the basic emotion class, such as happiness, sadness, and anger, of the person in a given facial image. Researches have made substantial progress in the past few years, benefiting from the development of more powerful deep neural networks, such as ResNet (He et al. 2016) and Vision Transformer (Dosovitskiy et al. 2020), and more representative benchmarks, such as AffencNet (Mollahosseini, Hasani, and Mahoor 2017)and RAF-DB (Li, Deng, and Du 2017).\n\n\n\nPredicting the discrete emotion class of a facial image is difficult due to the expression ambiguity, imbalanced and noisy emotion labels. Therefore, most of the state-of-the-art (SOTA) methods (Xue, Wang, and Guo 2021; Li et al. 2021;She et al. 2021; Hwang et al. 2022; Wang et al. 2020) commonly pretrain deep models on large scale and less noisy datasets, for instance, Ms-Celeb-1M (Guo et al. 2016) and even ImageNet, to gain better feature representations first and fine-tune deep models on specific FER datasets. Most recently, CNNs equipped with attention modules (Ma, Sun,and Li 2021; Xue, Wang, and Guo 2021) and ViT variants (Li et al. 2021) have been introduced into FER and obtained promising performance. However, whether a vanilla ViT without extra training samples from upstream tasks can achieve competitive performance has not been explored thus far. In this paper, we demonstrate that a vanilla ViT-base backbone pretrained by MAE method (He et al. 2022) without extra data is comparable to the SOTA work.\n\n## Our Approach \n\nWe initialize the original ViT as a Masked Autoencoder (MAE) (He et al. 2022) on AffectNet (Mollahosseini,Hasani, and Mahoor 2017) dataset such that it can extract a stronger representation of expressive semantic information. MAE pretraining force the ViT to learn the facial local and global structures (Chen et al. 2022). Different facial emotions often share one or more facial muscle movements.Thus, facial expressions with low occurrence frequencies,for example, fear, could benefit from the structure learning of frequent facial expressions, for example, happiness, during the pretraining.\n\n\n\nMAE. The masked autoencoder (He et al. 2022) employs an asymmetric encoder-decoder architecture to implement the self-supervised task by randomly masking the patches of input images and reconstructing missing pixels. It is experimentally demonstrated that MAE can learn a large number of visual semantics and has excellent performance in several downstream tasks. Inspired by this, we adopt MAE to pretrain a vanilla ViT with powerful expression semantic representation and narrow the gap between self-supervised task and downstream FER task \n\n\n\nDataset. The upstream pretraining task requires large\n\nscale data, which enables the model to have strong representational capailii.For R,AfctNt istelarstavailable in-the-wild dataset that contains more than one million face images collected from the Internet by keyword searching. Among them, about 400K images are manually labeled.We clean the dataset based on the label file and use OpenFace (Baltrusaitis et al. 2018) for face alignment, leaving 269,316 images for self-supervised training.\n\n## Experiment \n\n## I mplementation \n\nWe choose the ViT-base and do self-supervised pretraining on the AffectNet training set. The pretraining length is 300epochs and the other settings of pretraining and fine-tuning are same as MAE (He et al. 2022). Due to the tranining set of AffectNet is imbalanced whereas the validation set is balanced, oversampling strategy is used while fine-tuning. We train the ViT model on each FER training subset and then evaluate on each FER testing subset.\n\n\n\n## Result \n\nWe compare our method with two pretraining methods using supervised learning and MAE on ImageNet-1K in Table 1. Although AffectNet is almost 4× smaller than ImageNet1K, pretraining with MAE on AffectNet yields better fine-tuning results than both supervised learning and MAE pretraining methods on ImageNet-1K. Our method based on ViT-base achieves the best accuracy of 90.22%, 61.73% and 89.00% on RAF-DB, AffectNet and FER-Plus respectively without any data augmentation, which gives us a powerful pretraining model for the subsequent work. And the performance reaches 91.07% on RAF-DB, 62.42% on AfectNet,and 90.18% on FER-Plus if regular augmentation is used (random horizontal flipping, color jitter and affine transformation).\n\n\n\n<div style=\"text-align: center;\">Table 1: MAE pretraining vs. supervised pretraining, numbers evaluated by fine-tuning the ViT-base weights from ImageNet-1K and AffectNet on downstream FER datasets without data augmentation. </div>\n\n\n\n<div style=\"text-align: center;\"><html><body><table border=\"1\"><thead><tr><td>method</td><td>pretrain data</td><td>RAF-DB</td><td>AffectNet</td></tr></thead><tbody><tr><td>Supervised</td><td>IN1K</td><td>85.14%</td><td>57.10%</td></tr><tr><td>MAE</td><td>IN1K</td><td>88.49%</td><td>60.87%</td></tr><tr><td>MAE</td><td>AffectNet</td><td>90.22%</td><td>61.73%</td></tr></tbody></table></body></html></div>\n\n\n<div style=\"text-align: center;\">Table 2: More ViT-based results on RAF-DB, AffectNet and FER-Plus. Here, data augmentation is employed. The variants, ViT-small and ViT-tiny are proposed in DeiT (Touvron et al. 2021). </div>\n\n\n\n<div style=\"text-align: center;\"><html><body><table border=\"1\"><thead><tr><td>method</td><td>pretrain data</td><td>ViT variants</td><td>RAF-DB</td><td>AffectNet</td><td>FER-Plus</td></tr></thead><tbody><tr><td>Supervised</td><td>IN1K</td><td>ViT-samll</td><td>87.19%</td><td>57.91%</td><td>88.46%</td></tr><tr><td>Supervised</td><td>IN1K</td><td>ViT-tiny</td><td>87.03%</td><td>58.28%</td><td>88.56%</td></tr><tr><td>Supervised</td><td>IN1K</td><td>ViT-base</td><td>87.22%</td><td>57.99%</td><td>88.91%</td></tr><tr><td>MAE</td><td>AffectNet</td><td>Vit-base</td><td>91.07%</td><td>62.42%</td><td>90.18%</td></tr></tbody></table></body></html></div>\n\n\nIn addition, to demonstrate the effectiveness of our method, we compare it with the existing state-of-the-art methods. In Table2, SCN (Wang et al. 2020) and RUL (Zhang, Wang, and Deng 2021) adopt ResNet as the backbone and introduce different uncertainty learning methods to tackle the uncertainty problem in FER. MVT (Li et al.2021) proposes the combination of grid-wise local attention and token-based global attention to overcome the weakness of CNN-based in long-range relation modeling. Compared with these well-designed SOTA methods, our results on the three FER datasets demonstrate the effectiveness and scalability of our method.\n\n\n\n<div style=\"text-align: center;\">Table 3: Comparsion with the state of the art on RAF-DB,AffectNet and FER-Plus. </div>\n\n\n\n<div style=\"text-align: center;\"><html><body><table border=\"1\"><thead><tr><td>method</td><td>year</td><td>RAF-DB</td><td>AffectNet</td><td>FER-Plus</td></tr></thead><tbody><tr><td>SCN (Wang et al. 2020)</td><td>2020</td><td>88.14%</td><td>60.23%</td><td>89.35%</td></tr><tr><td>RUL (Zhang, Wang, and Deng 2021)</td><td>2021</td><td>88.98%</td><td>61.43%</td><td></td></tr><tr><td>MVT (Li et al. 2021)</td><td>2021</td><td>88.26%</td><td>61.40%</td><td>89.22%</td></tr><tr><td>Ours</td><td>2022</td><td>91.07%</td><td>62.42%</td><td>90.18%</td></tr></tbody></table></body></html></div>\n\n\n## Conclusions \n\nIn this paper, we demonstrate that a vanilla ViT-based backbone pretrained by MAE method (He et al. 2022) without extra data is comparable to the SOTA work. The presented method achieves 91.07% on RAF-DB, 62.42% on AfectNet,and 90.18% on FER-Plus, which can serve as a transparent ViT-based baseline for FER studies.\n\n\n\n## References \n\nBaltrusaitis, T.; Zadeh, A.; Lim, Y. C.; and Morency, L.-P. 2018.Openface 2.0: Facial behavior analysis toolkit. In 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018), 59–66. IEEE.\nChen, X.; Ding, M.; Wang, X.; Xin, Y.;Mo, S.;Wang,Y.; Han,S.; Luo,P.; Zeng,G.; and Wang, J. 2022. Context autoencoder for self-supervised representation learning. arXiv preprint arXiv:2202.03026.\nDosovitskiy,A.;Beyer,L.;Kolesnikov,A.;Weissenborn,D.;Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer,M.; Heigold,G.:Gelly,S.;et al. 2020. An image is worth 16x16words:Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\nGong, W.; An, Z.; and Elfiky, N. M. 2022. Deep learning-based microexpression recognition: a survey. Neural Computing and Applications 1–24.\nGuo, Y.; Zhang, L.; Hu, Y.; He, X.; and Gao, J. 2016. Ms-celeb1m: A dataset and benchmark for large-scale face recognition. In European conference on computer vision,, 87–102. Springer.He, K.; Zhang, X.; Ren, S.; and Sun, J. 201. Deep residual learning for image recognition. In Proceedings of the EEE conference on computer vision and pattern recognition, 770–778.He, K.; Chen, X.; Xie, S.; Li,Y.; Dollár, P.; and Girshick, R. 2022.Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 16000–16009.\nHwang, H.; Kim,S.; Park, W.-J.; Seo,J.; Ko, K.; and Yeo, H. 2022.Vision transformer equipped with neural resizer on facial expression recognition task. In ICASSP 2022-2022 IEEE International \n\nConferenceAcouticecndgalcesinCASSP),2614–2618. IEEE.\nLi,H.; Sui, M; Zhao,F;Zha,Z.;and Wu,F.2021.Mvt: mask vision preprint arXiv:2106.04520.\nLi,S.; Deng, W; and Du,J.2017. Rliablecrowdsourcing and dep wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2852–2861.\nMa,F;un.nLi.21.Fclxogto with visual transformers and atentional selectivefusion.IEEE Transactions on Affective Computing.\nMollahossei,A.; Haai,B.;andMaoorM.H.2017.ffectnet:A thewild.EaaciononfctiveComputi):18–31.She,J.;Hu,Y.;,H.;Wa,J.;Q.;ad,T..Dive intoambigut:iii rucr estimationfor facial expression recogition.In roceedingsof e IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6248–6257.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles, A.;and Jégou, H. 2021. Training data-efficient image transformers & distillation through attention. In International conferenceon machine learning, 10347–10357. PMLR.\nWang, K.; Peng, X.;Yang,J.; Lu,S.; and Qiao,Y.2020.Suess ing uncertainties for large-scale facial expression recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 6897–6906.\nXue, F.; Wang, Q.; and Guo, G. 2021. Transfer: Learning relationaware facial expression representations with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 3601–3610.\nZhang, Y.; Wang, C.; and Deng, W. 2021. Relative uncertainty learning for facial expression recognition. Advances in Neural Information Processing Systems 34.\nZhao, Z.; Liu, Q.; and Zhou, F. 2021. Robust lightweight facial expressioncognitionetwork withlabel diibutiontraning.In Proceedingsof the AAAlconferenceonartificial intelligence, volume 35, 3510–3519.\n"
}